{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "—————1—————\n",
      "HashedCategoricalColumn(key='department', hash_bucket_size=10, dtype=tf.string)\n",
      "——————2————\n",
      "[HashedCategoricalColumn(key='department', hash_bucket_size=10, dtype=tf.string)]\n",
      "[[ 0.38959482 -0.30834228  0.14047201 -0.4914966  -0.3793363   0.16128609\n",
      "  -0.19833061 -0.31927118 -0.02395553  0.44846094]\n",
      " [ 0.38959482 -0.30834228  0.14047201 -0.4914966  -0.3793363   0.16128609\n",
      "  -0.19833061 -0.31927118 -0.02395553  0.44846094]\n",
      " [-0.44056746  0.40502527 -0.27313593  0.0972157  -0.32437634  0.16520736\n",
      "  -0.20840074  0.03475686  0.27677065 -0.3123621 ]\n",
      " [ 0.38959482 -0.30834228  0.14047201 -0.4914966  -0.3793363   0.16128609\n",
      "  -0.19833061 -0.31927118 -0.02395553  0.44846094]\n",
      " [-0.09558015 -0.01415543 -0.38759276 -0.24352945 -0.07161801 -0.39037183\n",
      "   0.3871828  -0.33830363 -0.13601029 -0.33882076]\n",
      " [ 0.15276995 -0.38286322  0.02860998 -0.17966697  0.46105486  0.09873489\n",
      "   0.07036567  0.11399277  0.05075244 -0.32522264]\n",
      " [ 0.36032566  0.1652059  -0.57469493  0.03589788 -0.31461325  0.07343267\n",
      "   0.17240639  0.23588552 -0.2826552   0.22143611]\n",
      " [ 0.36032566  0.1652059  -0.57469493  0.03589788 -0.31461325  0.07343267\n",
      "   0.17240639  0.23588552 -0.2826552   0.22143611]\n",
      " [ 0.38959482 -0.30834228  0.14047201 -0.4914966  -0.3793363   0.16128609\n",
      "  -0.19833061 -0.31927118 -0.02395553  0.44846094]\n",
      " [-0.0752562   0.02359157 -0.07766997  0.06144851 -0.38412586  0.06835458\n",
      "  -0.01540083 -0.04892922 -0.03389747 -0.19699635]\n",
      " [ 0.36032566  0.1652059  -0.57469493  0.03589788 -0.31461325  0.07343267\n",
      "   0.17240639  0.23588552 -0.2826552   0.22143611]\n",
      " [ 0.15276995 -0.38286322  0.02860998 -0.17966697  0.46105486  0.09873489\n",
      "   0.07036567  0.11399277  0.05075244 -0.32522264]\n",
      " [-0.44056746  0.40502527 -0.27313593  0.0972157  -0.32437634  0.16520736\n",
      "  -0.20840074  0.03475686  0.27677065 -0.3123621 ]]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "sess=tf.Session()\n",
    "#特征数据\n",
    "features = {\n",
    "    'department': ['sport', 'sport', 'draw', 'gard', 'trav', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'],\n",
    "}\n",
    "#特征列\n",
    "department = tf.feature_column.categorical_column_with_hash_bucket('department', 10, dtype=tf.string)\n",
    "print \"—————1—————\"\n",
    "print department\n",
    "print \"——————2————\"\n",
    "#组合特征列\n",
    "columns = [department]\n",
    "print columns\n",
    "columns = tf.feature_column.shared_embedding_columns(columns, dimension=10)\n",
    "#输入层（数据，特征列）\n",
    "inputs = tf.feature_column.input_layer(features, columns)\n",
    "#初始化并运行\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(tf.tables_initializer())\n",
    "sess.run(init)\n",
    " \n",
    "v=sess.run(inputs)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use input_layer________________________________________\n",
      "[array([[0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 2., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 2., 0.]],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.estimator.inputs import numpy_io\n",
    "import numpy as np\n",
    "import collections\n",
    "from tensorflow.python.framework import errors\n",
    "from tensorflow.python.platform import test\n",
    "from tensorflow.python.training import coordinator\n",
    "from tensorflow import feature_column\n",
    "\n",
    "from tensorflow.python.feature_column.feature_column import _LazyBuilder\n",
    "\n",
    "def test_numeric():\n",
    "\n",
    "    price = {'price': [[1.], [2.], [3.], [4.]]}  # 4行样本\n",
    "    builder = _LazyBuilder(price)\n",
    "\n",
    "    def transform_fn(x):\n",
    "        return x + 2\n",
    "    #normalizer_fn: 对该特征下的所有数据进行转换\n",
    "    price_column = feature_column.numeric_column('price', normalizer_fn=transform_fn)\n",
    "    #使用_LazyBuilder和inpu_layer来分别进行了测试.效果是一样的.\n",
    "    price_transformed_tensor = price_column._get_dense_tensor(builder)\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        print(session.run([price_transformed_tensor]))\n",
    "\n",
    "    # 使用input_layer\n",
    "\n",
    "    price_transformed_tensor = feature_column.input_layer(price, [price_column])\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        print('use input_layer' + '_' * 40)\n",
    "        print(session.run([price_transformed_tensor]))\n",
    "        \n",
    "#test_numeric()\n",
    "\n",
    "def test_bucketized_column():\n",
    "\n",
    "    price = {'price': [[5.], [15.], [25.], [35.]]}  # 4行样本\n",
    "\n",
    "    price_column = feature_column.numeric_column('price')\n",
    "    #source_column: 必须是numeric_column\n",
    "    #boundaries: 不同的桶。boundaries=[0., 1., 2.],产生的bucket就是, (-inf, 0.), [0., 1.), [1., 2.), and [2., +inf), 每\n",
    "    #一个区间分别表示0, 1, 2, 3,所以相当于分桶分了4个.\n",
    "    bucket_price = feature_column.bucketized_column(price_column, [0, 10, 20, 30, 40])\n",
    "\n",
    "    price_bucket_tensor = feature_column.input_layer(price, [bucket_price])\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        print(session.run([price_bucket_tensor]))\n",
    "        \n",
    "#我们看到分桶之后，会直接转换成one-hot形式的。\n",
    "#test_bucketized_column()\n",
    "\n",
    "def test_categorical_column_with_vocabulary_list():\n",
    "\n",
    "    color_data = {'color': [['R', 'R'], ['G', 'R'], ['B', 'G'], ['A', 'A']]}  # 4行样本\n",
    "\n",
    "    builder = _LazyBuilder(color_data)\n",
    "    #categorical_column_with_vocabulary_list来说返回的是sparser_tensor\n",
    "    #注意 id_tensor 这个是有效的，另外一个是None. 对于线性模型来说是可以直接使用sparser_tensor的\n",
    "    color_column = feature_column.categorical_column_with_vocabulary_list(\n",
    "        'color', ['R', 'G', 'B'], dtype=tf.string, default_value=-1\n",
    "    )\n",
    "\n",
    "    color_column_tensor = color_column._get_sparse_tensors(builder)\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "        session.run(tf.tables_initializer())\n",
    "\n",
    "        print(session.run([color_column_tensor.id_tensor]))\n",
    "    #对于深度模型来说，需要将sparser转换成dense，所以也就有了indicator_column 这个函数的出现。\n",
    "    # 将稀疏的转换成dense，也就是one-hot形式，只是multi-hot\n",
    "    #indicator_column的作用就是将category产生的sparser tensor转换成dense tensor.\n",
    "    color_column_identy = feature_column.indicator_column(color_column)\n",
    "    #input_layer: 只接受dense tensor\n",
    "    color_dense_tensor = feature_column.input_layer(color_data, [color_column_identy])\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        #tables_initializer: 在sparser的时候使用的，如果不进行初始化会出现 Table not initialized.\n",
    "        session.run(tf.tables_initializer())\n",
    "\n",
    "        print('use input_layer' + '_' * 40)\n",
    "        print(session.run([color_dense_tensor]))\n",
    "        \n",
    "#test_categorical_column_with_vocabulary_list()\n",
    "\n",
    "def test_categorical_column_with_hash_bucket():\n",
    "\n",
    "    color_data = {'color': [['R'], ['G'], ['B'], ['A']]}  # 4行样本\n",
    "    builder = _LazyBuilder(color_data)\n",
    "    #当category的数量很多，也就无法使用指定category的方法来处理了，那么，可以使用这种哈希分桶的方式来进行处理\n",
    "    #毕竟对于hash_bucket来说，对于bucket_size的选取是个问题\n",
    "    color_column = feature_column.categorical_column_with_hash_bucket('color', 7)\n",
    "    color_column_tensor = color_column._get_sparse_tensors(builder)\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.tables_initializer())\n",
    "        print(session.run([color_column_tensor.id_tensor]))\n",
    "\n",
    "    # 将稀疏的转换成dense，也就是one-hot形式，只是multi-hot\n",
    "    color_column_identy = feature_column.indicator_column(color_column)\n",
    "    color_dense_tensor = feature_column.input_layer(color_data, [color_column_identy])\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.tables_initializer())\n",
    "        print('use input_layer' + '_' * 40)\n",
    "        print(session.run([color_dense_tensor]))\n",
    "#从上面看这种hash分桶的方法，在hash_size的选择上是很重要的\n",
    "#现在选择3，对于R 和 B 来说分桶到一个烈面了；对于 G和A 分桶到一个里面了\n",
    "#当将 hash_size=7来测试, R G B A就都分到了不同的桶中，所以值越大也容易精确的分桶.\n",
    "\n",
    "#test_categorical_column_with_hash_bucket()\n",
    "\n",
    "def test_embedding():\n",
    "    color_data = {'color': [['R'], ['G'], ['B'], ['A']]}  # 4行样本\n",
    "\n",
    "    color_column = feature_column.categorical_column_with_vocabulary_list(\n",
    "        'color', ['R', 'G', 'B'], dtype=tf.string, default_value=-1\n",
    "    )\n",
    "    #每一个都会转换成5个维度的数据，并且使用高斯分布来进行初始化。\n",
    "    #因为A 没有在catergorical_column中出现，所以使用了0进行初始化.\n",
    "    color_embeding = feature_column.embedding_column(color_column, 5)\n",
    "    color_embeding_dense_tensor = feature_column.input_layer(color_data, [color_embeding])\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "        session.run(tf.tables_initializer())\n",
    "\n",
    "        print('embeding' + '_' * 40)\n",
    "        print(session.run([color_embeding_dense_tensor]))\n",
    "        \n",
    "#test_embedding()\n",
    "\n",
    "def test_weighted_categorical_column():\n",
    "    color_data = {'color': [['R'], ['G'], ['B'], ['A']],\n",
    "                  'weight': [[1.0], [2.0], [4.0], [8.0]]}  # 4行样本\n",
    "\n",
    "    color_column = feature_column.categorical_column_with_vocabulary_list(\n",
    "        'color', ['R', 'G', 'B'], dtype=tf.string, default_value=-1\n",
    "    )\n",
    "\n",
    "    color_weight_categorical_column = feature_column.weighted_categorical_column(color_column, 'weight')\n",
    "\n",
    "    builder = _LazyBuilder(color_data)\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        id_tensor, weight = color_weight_categorical_column._get_sparse_tensors(builder)\n",
    "\n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "        session.run(tf.tables_initializer())\n",
    "\n",
    "        print('weighted categorical' + '-' * 40)\n",
    "\n",
    "        print(session.run([id_tensor]))\n",
    "        print('-' * 40)\n",
    "        print(session.run([weight]))\n",
    "\n",
    "#test_weighted_categorical_column()\n",
    "\n",
    "#对所有特征进行线性加权操作\n",
    "def get_linear_model_bias():\n",
    "    with tf.variable_scope('linear_model', reuse=True):\n",
    "        return tf.get_variable('bias_weights')\n",
    "def get_linear_model_column_var(column):\n",
    "    return tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                             'linear_model/' + column.name)[0]\n",
    "def test_linear_model():\n",
    "    \"\"\"\n",
    "    测试线性模型\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    featrues = {\n",
    "        'price': [[1.0], [5.0], [10.0]],\n",
    "        'color': [['R'], ['G'], ['B']]\n",
    "    }\n",
    "    price_column = feature_column.numeric_column('price')\n",
    "    color_column = feature_column.categorical_column_with_vocabulary_list('color', ['R', 'G', 'B'])\n",
    "    prediction = feature_column.linear_model(featrues, [price_column, color_column])\n",
    "\n",
    "    bias = get_linear_model_bias()\n",
    "    price_var = get_linear_model_column_var(price_column)\n",
    "    color_var = get_linear_model_column_var(color_column)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "\n",
    "        sess.run(bias.assign([7.0]))\n",
    "        sess.run(price_var.assign([[10.0]]))\n",
    "        sess.run(color_var.assign([[2.0], [2.0], [2.0]]))\n",
    "\n",
    "        predication_result = sess.run([prediction])\n",
    "\n",
    "        print(predication_result)\n",
    "\n",
    "#test_linear_model()\n",
    "\n",
    "def test_crossed_column():\n",
    "    \"\"\"\n",
    "    crossed column测试\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    featrues = {\n",
    "        'price': [['A', 'A'], ['B', 'D'], ['C', 'A']],\n",
    "        'color': [['R', 'R'], ['G', 'G'], ['B', 'B']]\n",
    "    }\n",
    "\n",
    "    price = feature_column.categorical_column_with_vocabulary_list('price',\n",
    "                                                                   ['A', 'B', 'C', 'D'])\n",
    "    color = feature_column.categorical_column_with_vocabulary_list('color',\n",
    "                                                                   ['R', 'G', 'B'])\n",
    "    p_x_c = feature_column.crossed_column([price, color], 16)\n",
    "\n",
    "    p_x_c_identy = feature_column.indicator_column(p_x_c)\n",
    "\n",
    "    p_x_c_identy_dense_tensor = feature_column.input_layer(featrues, [p_x_c_identy])\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "        session.run(tf.tables_initializer())\n",
    "\n",
    "        print('use input_layer' + '_' * 40)\n",
    "        print(session.run([p_x_c_identy_dense_tensor]))\n",
    "\n",
    "test_crossed_column()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
