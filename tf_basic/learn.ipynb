{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 3.]\n",
      " [0. 0. 3.]\n",
      " [0. 0. 3.]]\n",
      "2\n",
      "[2 4 6]\n",
      "[[2. 4. 6.]\n",
      " [2. 4. 6.]\n",
      " [2. 4. 6.]]\n",
      "[[0. 0. 6.]\n",
      " [0. 0. 6.]\n",
      " [0. 0. 6.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  \n",
    "\n",
    "#两个矩阵的对应元素各自相乘！！\n",
    "x=tf.constant([[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]])  \n",
    "y=tf.constant([[0,0,1.0],[0,0,1.0],[0,0,1.0]])\n",
    "#注意这里这里x,y要有相同的数据类型，不然就会因为数据类型不匹配而出错\n",
    "z=tf.multiply(x,y)\n",
    "\n",
    "#两个数相乘\n",
    "x1=tf.constant(1)\n",
    "y1=tf.constant(2)\n",
    "#注意这里这里x1,y1要有相同的数据类型，不然就会因为数据类型不匹配而出错\n",
    "z1=tf.multiply(x1,y1)\n",
    "\n",
    "#两个数相乘\n",
    "x1=tf.constant([1,2,3])\n",
    "y1=tf.constant([2,2,2])\n",
    "#注意这里这里x1,y1要有相同的数据类型，不然就会因为数据类型不匹配而出错\n",
    "z0=tf.multiply(x1,y1)\n",
    "\n",
    "#数和矩阵相乘\n",
    "x2=tf.constant([[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]])\n",
    "y2=tf.constant(2.0)\n",
    "#注意这里这里x1,y1要有相同的数据类型，不然就会因为数据类型不匹配而出错\n",
    "z2=tf.multiply(x2,y2)\n",
    "\n",
    "#两个矩阵相乘\n",
    "x3=tf.constant([[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]])  \n",
    "y3=tf.constant([[0,0,1.0],[0,0,1.0],[0,0,1.0]])\n",
    "#注意这里这里x,y要满足矩阵相乘的格式要求。\n",
    "z3=tf.matmul(x,y)\n",
    "\n",
    "with tf.Session() as sess:  \n",
    "    print(sess.run(z))\n",
    "    print(sess.run(z1))\n",
    "    print(sess.run(z0))\n",
    "    print(sess.run(z2))\n",
    "    print(sess.run(z3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0\n",
      "[2. 4. 6.]\n",
      "[6. 6.]\n",
      "[[6.]\n",
      " [6.]]\n",
      "[6. 6.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    " \n",
    "x = [[1,2,3],\n",
    "     [1,2,3]]\n",
    " \n",
    "xx = tf.cast(x,tf.float32)\n",
    "\n",
    "#xxs = tf.sigmoid(xx)\n",
    "xxs = tf.reduce_sum(xx, axis=1)\n",
    " \n",
    "sum_all = tf.reduce_sum(xx, keepdims=False)\n",
    "sum_0 = tf.reduce_sum(xx, axis=0, keepdims=False)\n",
    "sum_1 = tf.reduce_sum(xx, axis=1, keepdims=False)\n",
    "sum_2 = tf.reduce_sum(xx, axis=1, keepdims=True)\n",
    " \n",
    "#如果axis为None，input_tensor的维度降为1\n",
    "#如果axis为0，矩阵按行计算，维度降低一个维度\n",
    "#如果axis为1，矩阵按列计算，维度降低一个维度\n",
    "#如果keep_dims==true，保持维度不变，矩阵按行或者按列计算\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    s_a,s_0,s_1,s_2, xxx = sess.run([sum_all, sum_0, sum_1, sum_2, xxs])\n",
    " \n",
    "print(s_a)\n",
    "print(s_0)\n",
    "print(s_1)\n",
    "print(s_2)\n",
    "print(xxx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"DecodeCSV:5\", shape=(), dtype=string)\n",
      "Tensor(\"DecodeCSV_1:5\", shape=(), dtype=string)\n",
      "Tensor(\"DecodeCSV_2:5\", shape=(), dtype=string)\n",
      "Tensor(\"DecodeCSV_3:5\", shape=(), dtype=string)\n",
      "ok\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Unquoted fields cannot have quotes/CRLFs inside\n\t [[node DecodeCSV_3 (defined at <ipython-input-1-fe7029a56e55>:9) ]]\n\nOriginal stack trace for u'DecodeCSV_3':\n  File \"/usr/lib64/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib64/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/lib/python2.7/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/usr/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"/usr/lib64/python2.7/site-packages/tornado/ioloop.py\", line 1073, in start\n    handler_func(fd_obj, events)\n  File \"/usr/lib64/python2.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib64/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 456, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib64/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 486, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib64/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 438, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib64/python2.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2714, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2818, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2878, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-fe7029a56e55>\", line 9, in <module>\n    cols = tf.decode_csv(line, ['', '', '0', '', '0', '0', '200', '',''], field_delim='\\t')\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/parsing_ops.py\", line 1962, in decode_csv\n    na_value, select_cols, name\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/parsing_ops.py\", line 2021, in decode_csv_v2\n    select_cols=select_cols,\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_parsing_ops.py\", line 103, in decode_csv\n    na_value=na_value, select_cols=select_cols, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fe7029a56e55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1368\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Unquoted fields cannot have quotes/CRLFs inside\n\t [[node DecodeCSV_3 (defined at <ipython-input-1-fe7029a56e55>:9) ]]\n\nOriginal stack trace for u'DecodeCSV_3':\n  File \"/usr/lib64/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib64/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/lib/python2.7/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/usr/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"/usr/lib64/python2.7/site-packages/tornado/ioloop.py\", line 1073, in start\n    handler_func(fd_obj, events)\n  File \"/usr/lib64/python2.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib64/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 456, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib64/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 486, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib64/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 438, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib64/python2.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2714, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2818, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2878, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-fe7029a56e55>\", line 9, in <module>\n    cols = tf.decode_csv(line, ['', '', '0', '', '0', '0', '200', '',''], field_delim='\\t')\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/parsing_ops.py\", line 1962, in decode_csv\n    na_value, select_cols, name\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/parsing_ops.py\", line 2021, in decode_csv_v2\n    select_cols=select_cols,\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_parsing_ops.py\", line 103, in decode_csv\n    na_value=na_value, select_cols=select_cols, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "count = 0\n",
    "global_val = \"\"\n",
    "with open(\"/data0/guolei7/data/data_train.txt\") as f:\n",
    "    for line in f:\n",
    "        count += 1\n",
    "        if count == 5:\n",
    "            break\n",
    "        cols = tf.decode_csv(line, ['', '', '0', '', '0', '0', '200', '',''], field_delim='\\t')\n",
    "        global_val = cols[5]\n",
    "        print(cols[5])\n",
    "    print(\"ok\")\n",
    "        \n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(global_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "—————1—————\n",
      "HashedCategoricalColumn(key='department', hash_bucket_size=10, dtype=tf.string)\n",
      "——————2————\n",
      "[HashedCategoricalColumn(key='department', hash_bucket_size=10, dtype=tf.string)]\n",
      "[[ 0.38959482 -0.30834228  0.14047201 -0.4914966  -0.3793363   0.16128609\n",
      "  -0.19833061 -0.31927118 -0.02395553  0.44846094]\n",
      " [ 0.38959482 -0.30834228  0.14047201 -0.4914966  -0.3793363   0.16128609\n",
      "  -0.19833061 -0.31927118 -0.02395553  0.44846094]\n",
      " [-0.44056746  0.40502527 -0.27313593  0.0972157  -0.32437634  0.16520736\n",
      "  -0.20840074  0.03475686  0.27677065 -0.3123621 ]\n",
      " [ 0.38959482 -0.30834228  0.14047201 -0.4914966  -0.3793363   0.16128609\n",
      "  -0.19833061 -0.31927118 -0.02395553  0.44846094]\n",
      " [-0.09558015 -0.01415543 -0.38759276 -0.24352945 -0.07161801 -0.39037183\n",
      "   0.3871828  -0.33830363 -0.13601029 -0.33882076]\n",
      " [ 0.15276995 -0.38286322  0.02860998 -0.17966697  0.46105486  0.09873489\n",
      "   0.07036567  0.11399277  0.05075244 -0.32522264]\n",
      " [ 0.36032566  0.1652059  -0.57469493  0.03589788 -0.31461325  0.07343267\n",
      "   0.17240639  0.23588552 -0.2826552   0.22143611]\n",
      " [ 0.36032566  0.1652059  -0.57469493  0.03589788 -0.31461325  0.07343267\n",
      "   0.17240639  0.23588552 -0.2826552   0.22143611]\n",
      " [ 0.38959482 -0.30834228  0.14047201 -0.4914966  -0.3793363   0.16128609\n",
      "  -0.19833061 -0.31927118 -0.02395553  0.44846094]\n",
      " [-0.0752562   0.02359157 -0.07766997  0.06144851 -0.38412586  0.06835458\n",
      "  -0.01540083 -0.04892922 -0.03389747 -0.19699635]\n",
      " [ 0.36032566  0.1652059  -0.57469493  0.03589788 -0.31461325  0.07343267\n",
      "   0.17240639  0.23588552 -0.2826552   0.22143611]\n",
      " [ 0.15276995 -0.38286322  0.02860998 -0.17966697  0.46105486  0.09873489\n",
      "   0.07036567  0.11399277  0.05075244 -0.32522264]\n",
      " [-0.44056746  0.40502527 -0.27313593  0.0972157  -0.32437634  0.16520736\n",
      "  -0.20840074  0.03475686  0.27677065 -0.3123621 ]]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "sess=tf.Session()\n",
    "#特征数据\n",
    "features = {\n",
    "    'department': ['sport', 'sport', 'draw', 'gard', 'trav', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'],\n",
    "}\n",
    "#特征列\n",
    "department = tf.feature_column.categorical_column_with_hash_bucket('department', 10, dtype=tf.string)\n",
    "print \"—————1—————\"\n",
    "print department\n",
    "print \"——————2————\"\n",
    "#组合特征列\n",
    "columns = [department]\n",
    "print columns\n",
    "columns = tf.feature_column.shared_embedding_columns(columns, dimension=10)\n",
    "#输入层（数据，特征列）\n",
    "inputs = tf.feature_column.input_layer(features, columns)\n",
    "#初始化并运行\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(tf.tables_initializer())\n",
    "sess.run(init)\n",
    " \n",
    "v=sess.run(inputs)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use input_layer________________________________________\n",
      "[array([[0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 2., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 2., 0.]],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.estimator.inputs import numpy_io\n",
    "import numpy as np\n",
    "import collections\n",
    "from tensorflow.python.framework import errors\n",
    "from tensorflow.python.platform import test\n",
    "from tensorflow.python.training import coordinator\n",
    "from tensorflow import feature_column\n",
    "\n",
    "from tensorflow.python.feature_column.feature_column import _LazyBuilder\n",
    "\n",
    "def test_numeric():\n",
    "\n",
    "    price = {'price': [[1.], [2.], [3.], [4.]]}  # 4行样本\n",
    "    builder = _LazyBuilder(price)\n",
    "\n",
    "    def transform_fn(x):\n",
    "        return x + 2\n",
    "    #normalizer_fn: 对该特征下的所有数据进行转换\n",
    "    price_column = feature_column.numeric_column('price', normalizer_fn=transform_fn)\n",
    "    #使用_LazyBuilder和inpu_layer来分别进行了测试.效果是一样的.\n",
    "    price_transformed_tensor = price_column._get_dense_tensor(builder)\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        print(session.run([price_transformed_tensor]))\n",
    "\n",
    "    # 使用input_layer\n",
    "\n",
    "    price_transformed_tensor = feature_column.input_layer(price, [price_column])\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        print('use input_layer' + '_' * 40)\n",
    "        print(session.run([price_transformed_tensor]))\n",
    "        \n",
    "#test_numeric()\n",
    "\n",
    "def test_bucketized_column():\n",
    "\n",
    "    price = {'price': [[5.], [15.], [25.], [35.]]}  # 4行样本\n",
    "\n",
    "    price_column = feature_column.numeric_column('price')\n",
    "    #source_column: 必须是numeric_column\n",
    "    #boundaries: 不同的桶。boundaries=[0., 1., 2.],产生的bucket就是, (-inf, 0.), [0., 1.), [1., 2.), and [2., +inf), 每\n",
    "    #一个区间分别表示0, 1, 2, 3,所以相当于分桶分了4个.\n",
    "    bucket_price = feature_column.bucketized_column(price_column, [0, 10, 20, 30, 40])\n",
    "\n",
    "    price_bucket_tensor = feature_column.input_layer(price, [bucket_price])\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        print(session.run([price_bucket_tensor]))\n",
    "        \n",
    "#我们看到分桶之后，会直接转换成one-hot形式的。\n",
    "#test_bucketized_column()\n",
    "\n",
    "def test_categorical_column_with_vocabulary_list():\n",
    "\n",
    "    color_data = {'color': [['R', 'R'], ['G', 'R'], ['B', 'G'], ['A', 'A']]}  # 4行样本\n",
    "\n",
    "    builder = _LazyBuilder(color_data)\n",
    "    #categorical_column_with_vocabulary_list来说返回的是sparser_tensor\n",
    "    #注意 id_tensor 这个是有效的，另外一个是None. 对于线性模型来说是可以直接使用sparser_tensor的\n",
    "    color_column = feature_column.categorical_column_with_vocabulary_list(\n",
    "        'color', ['R', 'G', 'B'], dtype=tf.string, default_value=-1\n",
    "    )\n",
    "\n",
    "    color_column_tensor = color_column._get_sparse_tensors(builder)\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "        session.run(tf.tables_initializer())\n",
    "\n",
    "        print(session.run([color_column_tensor.id_tensor]))\n",
    "    #对于深度模型来说，需要将sparser转换成dense，所以也就有了indicator_column 这个函数的出现。\n",
    "    # 将稀疏的转换成dense，也就是one-hot形式，只是multi-hot\n",
    "    #indicator_column的作用就是将category产生的sparser tensor转换成dense tensor.\n",
    "    color_column_identy = feature_column.indicator_column(color_column)\n",
    "    #input_layer: 只接受dense tensor\n",
    "    color_dense_tensor = feature_column.input_layer(color_data, [color_column_identy])\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        #tables_initializer: 在sparser的时候使用的，如果不进行初始化会出现 Table not initialized.\n",
    "        session.run(tf.tables_initializer())\n",
    "\n",
    "        print('use input_layer' + '_' * 40)\n",
    "        print(session.run([color_dense_tensor]))\n",
    "        \n",
    "#test_categorical_column_with_vocabulary_list()\n",
    "\n",
    "def test_categorical_column_with_hash_bucket():\n",
    "\n",
    "    color_data = {'color': [['R'], ['G'], ['B'], ['A']]}  # 4行样本\n",
    "    builder = _LazyBuilder(color_data)\n",
    "    #当category的数量很多，也就无法使用指定category的方法来处理了，那么，可以使用这种哈希分桶的方式来进行处理\n",
    "    #毕竟对于hash_bucket来说，对于bucket_size的选取是个问题\n",
    "    color_column = feature_column.categorical_column_with_hash_bucket('color', 7)\n",
    "    color_column_tensor = color_column._get_sparse_tensors(builder)\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.tables_initializer())\n",
    "        print(session.run([color_column_tensor.id_tensor]))\n",
    "\n",
    "    # 将稀疏的转换成dense，也就是one-hot形式，只是multi-hot\n",
    "    color_column_identy = feature_column.indicator_column(color_column)\n",
    "    color_dense_tensor = feature_column.input_layer(color_data, [color_column_identy])\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.tables_initializer())\n",
    "        print('use input_layer' + '_' * 40)\n",
    "        print(session.run([color_dense_tensor]))\n",
    "#从上面看这种hash分桶的方法，在hash_size的选择上是很重要的\n",
    "#现在选择3，对于R 和 B 来说分桶到一个烈面了；对于 G和A 分桶到一个里面了\n",
    "#当将 hash_size=7来测试, R G B A就都分到了不同的桶中，所以值越大也容易精确的分桶.\n",
    "\n",
    "#test_categorical_column_with_hash_bucket()\n",
    "\n",
    "def test_embedding():\n",
    "    color_data = {'color': [['R'], ['G'], ['B'], ['A']]}  # 4行样本\n",
    "\n",
    "    color_column = feature_column.categorical_column_with_vocabulary_list(\n",
    "        'color', ['R', 'G', 'B'], dtype=tf.string, default_value=-1\n",
    "    )\n",
    "    #每一个都会转换成5个维度的数据，并且使用高斯分布来进行初始化。\n",
    "    #因为A 没有在catergorical_column中出现，所以使用了0进行初始化.\n",
    "    color_embeding = feature_column.embedding_column(color_column, 5)\n",
    "    color_embeding_dense_tensor = feature_column.input_layer(color_data, [color_embeding])\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "        session.run(tf.tables_initializer())\n",
    "\n",
    "        print('embeding' + '_' * 40)\n",
    "        print(session.run([color_embeding_dense_tensor]))\n",
    "        \n",
    "#test_embedding()\n",
    "\n",
    "def test_weighted_categorical_column():\n",
    "    color_data = {'color': [['R'], ['G'], ['B'], ['A']],\n",
    "                  'weight': [[1.0], [2.0], [4.0], [8.0]]}  # 4行样本\n",
    "\n",
    "    color_column = feature_column.categorical_column_with_vocabulary_list(\n",
    "        'color', ['R', 'G', 'B'], dtype=tf.string, default_value=-1\n",
    "    )\n",
    "\n",
    "    color_weight_categorical_column = feature_column.weighted_categorical_column(color_column, 'weight')\n",
    "\n",
    "    builder = _LazyBuilder(color_data)\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        id_tensor, weight = color_weight_categorical_column._get_sparse_tensors(builder)\n",
    "\n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "        session.run(tf.tables_initializer())\n",
    "\n",
    "        print('weighted categorical' + '-' * 40)\n",
    "\n",
    "        print(session.run([id_tensor]))\n",
    "        print('-' * 40)\n",
    "        print(session.run([weight]))\n",
    "\n",
    "#test_weighted_categorical_column()\n",
    "\n",
    "#对所有特征进行线性加权操作\n",
    "def get_linear_model_bias():\n",
    "    with tf.variable_scope('linear_model', reuse=True):\n",
    "        return tf.get_variable('bias_weights')\n",
    "def get_linear_model_column_var(column):\n",
    "    return tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                             'linear_model/' + column.name)[0]\n",
    "def test_linear_model():\n",
    "    \"\"\"\n",
    "    测试线性模型\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    featrues = {\n",
    "        'price': [[1.0], [5.0], [10.0]],\n",
    "        'color': [['R'], ['G'], ['B']]\n",
    "    }\n",
    "    price_column = feature_column.numeric_column('price')\n",
    "    color_column = feature_column.categorical_column_with_vocabulary_list('color', ['R', 'G', 'B'])\n",
    "    prediction = feature_column.linear_model(featrues, [price_column, color_column])\n",
    "\n",
    "    bias = get_linear_model_bias()\n",
    "    price_var = get_linear_model_column_var(price_column)\n",
    "    color_var = get_linear_model_column_var(color_column)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "\n",
    "        sess.run(bias.assign([7.0]))\n",
    "        sess.run(price_var.assign([[10.0]]))\n",
    "        sess.run(color_var.assign([[2.0], [2.0], [2.0]]))\n",
    "\n",
    "        predication_result = sess.run([prediction])\n",
    "\n",
    "        print(predication_result)\n",
    "\n",
    "#test_linear_model()\n",
    "\n",
    "def test_crossed_column():\n",
    "    \"\"\"\n",
    "    crossed column测试\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    featrues = {\n",
    "        'price': [['A', 'A'], ['B', 'D'], ['C', 'A']],\n",
    "        'color': [['R', 'R'], ['G', 'G'], ['B', 'B']]\n",
    "    }\n",
    "\n",
    "    price = feature_column.categorical_column_with_vocabulary_list('price',\n",
    "                                                                   ['A', 'B', 'C', 'D'])\n",
    "    color = feature_column.categorical_column_with_vocabulary_list('color',\n",
    "                                                                   ['R', 'G', 'B'])\n",
    "    p_x_c = feature_column.crossed_column([price, color], 16)\n",
    "\n",
    "    p_x_c_identy = feature_column.indicator_column(p_x_c)\n",
    "\n",
    "    p_x_c_identy_dense_tensor = feature_column.input_layer(featrues, [p_x_c_identy])\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "        session.run(tf.tables_initializer())\n",
    "\n",
    "        print('use input_layer' + '_' * 40)\n",
    "        print(session.run([p_x_c_identy_dense_tensor]))\n",
    "\n",
    "test_crossed_column()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
